{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obbwln6HybAB"
   },
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BTqSZbtybAD"
   },
   "source": [
    "\n",
    "\n",
    "# **Predicting a song's populatiry on Spotify with Random Forrests<br><br>**\n",
    " \n",
    "  \n",
    "\n",
    "      **Justine Ferret, Arthur Guyart et Yanis Martinet**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym2-ARncybAE"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:00:03.577077Z",
     "start_time": "2021-03-31T10:00:03.541835Z"
    },
    "id": "DihfDSO8ybAE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T11:03:06.624882Z",
     "start_time": "2021-03-31T11:03:06.565449Z"
    },
    "id": "ouXc9OFSybAE",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_ccallback_c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-67a52d0d3739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# custom module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0memlyon_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0memlyon_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ML-Spotify/notebooks/emlyon_module/imports.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepreload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbcolz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn_pandas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_pep440\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ccallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLowLevelCallable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/_lib/_ccallback.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ccallback_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPyCFuncPtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFUNCTYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__bases__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_ccallback_c'"
     ]
    }
   ],
   "source": [
    "# base modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "# custom module\n",
    "from emlyon_module.imports import *\n",
    "from emlyon_module.structured import *\n",
    "\n",
    "# for manipulating data\n",
    "from pandas_summary import DataFrameSummary\n",
    "!pip install dill\n",
    "import dill\n",
    "\n",
    "# for Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# for visualization\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "!pip install -U plotnine\n",
    "from plotnine import ggplot, aes\n",
    "from plotnine.stats import stat_smooth\n",
    "from pdpbox import pdp\n",
    "# plotly\n",
    "# seaborn\n",
    "# altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKJ6EAYhybAF"
   },
   "source": [
    "# **I) Data Inspection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gQf3vuFybAF"
   },
   "source": [
    "# a) Import the data\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:00:14.848639Z",
     "start_time": "2021-03-31T10:00:14.323640Z"
    },
    "id": "q92U-BPuybAF"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e997535b86be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath_to_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"release_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "path_to_repo = os.path.dirname(os.getcwd())\n",
    "\n",
    "path = os.path.join(path_to_repo, \"data\", \"\")\n",
    "\n",
    "path_to_datasets = os.path.join(path, 'data.csv')\n",
    "\n",
    "df_raw = pd.read_csv(path_to_datasets, low_memory = False, parse_dates = [\"release_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJtI-yUvybAG"
   },
   "source": [
    "<a id=\"look_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2qj1jvrybAG"
   },
   "source": [
    "# b) First look on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrEgkpiZ2OUx"
   },
   "source": [
    "First let's look the type of data and column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:00:16.419043Z",
     "start_time": "2021-03-31T10:00:16.376239Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IcjIiW8ybAG",
    "outputId": "3d8c3d87-63ac-42f7-e09f-d1e90c37834b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:00:17.077971Z",
     "start_time": "2021-03-31T10:00:17.020948Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "F285-QhYybAH",
    "outputId": "59bfbb8a-d0cf-48d8-96a3-f8adde3e5663",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKj7K1lW2Wb1"
   },
   "source": [
    "Let's have a quick look on the data repartition over each music characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1K2DiJ7xCA8e",
    "outputId": "18ab97d6-351d-4987-b839-9fc4e2d97091",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_raw.hist(figsize=(20, 20), color='Blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWeF0SbvKWoe"
   },
   "source": [
    "These graph shows us the unbalanced data which will need to be retreat through intervals. Therefore the unbalanced identied data are on the acousticsness and intrumentalness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24QE96y5JoKs"
   },
   "source": [
    "# c) Correlation Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_lJ9DAb2eGQ"
   },
   "source": [
    "Too have a first sigh of the possible correlation over the popularity and our dataset, we must sort the descending correlation from all our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Dys30GLJqak",
    "outputId": "85d12cc9-688b-4afc-f675-411860ba1ddb"
   },
   "outputs": [],
   "source": [
    "corr = np.abs(df_raw.corr())\n",
    "series = np.abs(corr['popularity']).sort_values(ascending=False)\n",
    "for i, row in enumerate(series):\n",
    "    if 0.1 <= row < 1:\n",
    "      print(f'{series.index[i]:17} --> {row: .2f} (abs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYUFFT8xKvNk"
   },
   "source": [
    "The year, acousticness and loudness appears to be the three main correlators to the popularity. For now artists do not seems correlate to popularity, this might result from the format of the artist column in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuIfv9-RybAR"
   },
   "source": [
    "# d) Inspection of our predicted variable - Popularity\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The variable we want to predict is the popularity of a song, based on its characteristics, artist and date of release. So let's have a look on the popularity data\n",
    "[texte du lien](https://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:24.967940Z",
     "start_time": "2021-03-31T10:02:24.916357Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-gVRM6jybAR",
    "outputId": "35e73c7b-bfbf-45c4-d882-9472174cec4c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_raw.popularity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:26.252334Z",
     "start_time": "2021-03-31T10:02:26.180581Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DYHPXU-4ybAR",
    "outputId": "384ddca6-de85-484c-9bd6-fd0e8e4a6295",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw[df_raw['popularity'] > 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa8HpDWoybAS"
   },
   "source": [
    "The popularity rating is a numerical value between 0 and 100 attributed to each song, 100 meaning that a song is very popular. Out of 173,292 total songs, only 31 have a popularity rating of 90 or above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR1UAwpXybAR"
   },
   "source": [
    "# **II) Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr2B6dIS2jpu"
   },
   "source": [
    "# A) **Preprocessing of Artists Name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd8vyfwWybAH"
   },
   "source": [
    "As we can see, the elements in the 'artists' column are actually sublists. For example on the row 174,384, the song has 3 different artists. In order to process the fact that a song could have multiple.\n",
    "*We will add a column for each secondary artist on a song so that the model can process each artist individualy instead of processing the list as a single element.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:16.607967Z",
     "start_time": "2021-03-31T10:00:18.905376Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wskWIPL8ybAH",
    "outputId": "f6dbbc6c-26b9-4e1b-ce35-51a4dd2b129c"
   },
   "outputs": [],
   "source": [
    "for i, l in enumerate(df_raw[\"artists\"]):\n",
    "    print(\"list\",i,\"is\",type(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25cAW3k3ybAL"
   },
   "source": [
    "The sublists aren't event seen as lists by Pandas in the first place but rather as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:17.708706Z",
     "start_time": "2021-03-31T10:01:16.793307Z"
    },
    "id": "mUAtZb_cybAM"
   },
   "outputs": [],
   "source": [
    "df_raw[\"artists\"] = df_raw[\"artists\"].\b\b\b\b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gxAWNUv28hf"
   },
   "source": [
    " Deleting a part of the artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCRK-G0zLWE5"
   },
   "source": [
    "*As the Artist columns is equal to the list of all the artist who engaged within the song, we must split these list and evaluate the great number or artist to keep.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:26.076582Z",
     "start_time": "2021-03-31T10:01:25.997967Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MXT_zpRybAM",
    "outputId": "c1acbcf9-1625-4c8a-dc7e-42c7a9f25cbe"
   },
   "outputs": [],
   "source": [
    "max(len(l) for l in df_raw['artists'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkxONcQyybAM"
   },
   "source": [
    "The song with the most recording artists on this dataset has 28 different people on it. A simpler way to deal with that might be to cut the rows with too many artists on a single song so that we won't create too many columns that won't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:26.922448Z",
     "start_time": "2021-03-31T10:01:26.844057Z"
    },
    "id": "vXUEB10MybAM"
   },
   "outputs": [],
   "source": [
    "size = []\n",
    "for i in df_raw[\"artists\"]:\n",
    "    size.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:28.003767Z",
     "start_time": "2021-03-31T10:01:27.359741Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "danoEXbhybAM",
    "outputId": "2f6599d4-dd85-4790-86b5-f199ff2e90e0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in range(1,29):\n",
    "    print([n, sum(i >= n for i in size), \"{:.4%}\".format(sum(i >= n for i in size)/len(size))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqJGZ4GBybAN"
   },
   "source": [
    "Only 1,097 rows (0.6% of the dataset) have 6 or more artists, we'll drop these rows so that only 4 secondary artists columns will have to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:28.275038Z",
     "start_time": "2021-03-31T10:01:28.225643Z"
    },
    "id": "eUD-GnmLybAN"
   },
   "outputs": [],
   "source": [
    "df_raw['#artists'] = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:28.880224Z",
     "start_time": "2021-03-31T10:01:28.799972Z"
    },
    "id": "xl3pmbREybAN"
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw[df_raw['#artists'] <= 5]\n",
    "df_raw = df_raw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:29.267269Z",
     "start_time": "2021-03-31T10:01:29.192171Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "tHJL488hybAN",
    "outputId": "712e67a0-6134-4c50-ffae-2804afdda834"
   },
   "outputs": [],
   "source": [
    "artists = pd.DataFrame.from_records(df_raw['artists'])\n",
    "artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC0gNc4k3T8p"
   },
   "source": [
    "#  Artists Column Subdivision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYP3YMzc3GqN"
   },
   "source": [
    "*We choose to split the list of the 5 artist into 5 different column to identify each of them. *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:33.115643Z",
     "start_time": "2021-03-31T10:01:33.020015Z"
    },
    "id": "hc_O4bdqybAN"
   },
   "outputs": [],
   "source": [
    "df_raw[\"main_artist\"] = artists[0]\n",
    "df_raw[\"artist2\"] = artists[1]\n",
    "df_raw[\"artist3\"] = artists[2]\n",
    "df_raw[\"artist4\"] = artists[3]\n",
    "df_raw[\"artist5\"] = artists[4]\n",
    "\n",
    "df_raw = df_raw.drop(columns=['artists'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:01:33.556136Z",
     "start_time": "2021-03-31T10:01:33.453839Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "thUprXShybAO",
    "outputId": "b4117ccf-29f7-4edd-bac8-8174992e4bcd",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2f1965556779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WNBsXdm9HlV"
   },
   "source": [
    "# **Creation of Artist Popularity Score** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHtG6eyY_ies"
   },
   "source": [
    "*To get a better model we choose to replace the artist name by a score of their popularity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Wu8GJh8_wuG"
   },
   "source": [
    "Therefore, we  created a fonction to attribute a popularity score to the  Artists of each music. For this we will take the mean popularity of the music with the Artist in it and then do the sum of artist popularity for the artists within the music.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_f_S28459SQU"
   },
   "outputs": [],
   "source": [
    "df_raw['mean_art_popularity'] = df_raw.groupby('main_artist')['popularity'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObqApRMv5DNA"
   },
   "outputs": [],
   "source": [
    "df_raw['mean_art2_popularity'] = df_raw.groupby('artist2')['popularity'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDnaWUms5Khn"
   },
   "outputs": [],
   "source": [
    "df_raw['mean_art3_popularity'] = df_raw.groupby('artist3')['popularity'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO1OXp9t5Mvt"
   },
   "outputs": [],
   "source": [
    "df_raw['mean_art4_popularity'] = df_raw.groupby('artist4')['popularity'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsRuXnWe5Ol4"
   },
   "outputs": [],
   "source": [
    "df_raw['mean_art5_popularity'] = df_raw.groupby('artist5')['popularity'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m56VnVvN6xjj"
   },
   "outputs": [],
   "source": [
    "df_raw['All_Artists_pop_sum'] = df_raw[['mean_art5_popularity','mean_art4_popularity','mean_art3_popularity','mean_art2_popularity','mean_art_popularity']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMu9kKHO7gz_"
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw.drop(labels=['mean_art5_popularity','mean_art4_popularity','mean_art3_popularity','mean_art2_popularity','mean_art_popularity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "yW9xFEixFUZH",
    "outputId": "23f16af6-7744-41b7-cc6f-53f9f73d8cf8"
   },
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqoiG-7UybAO"
   },
   "source": [
    "<a id=\"look_columns\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T1gb5KLybAO"
   },
   "source": [
    "# **B) Look at the musics characteristics  columns**\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHPXEmFZMdVN"
   },
   "source": [
    "To continue, we must first focus on the unbalanced data identified previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:00.554499Z",
     "start_time": "2021-03-31T10:02:00.514000Z"
    },
    "id": "_ykmtResybAO"
   },
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:02.486337Z",
     "start_time": "2021-03-31T10:02:02.119551Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "id": "Y35OYffIybAP",
    "outputId": "1221b34a-0255-4386-bd52-285cdf1699e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_all(df_raw.describe(include='all', datetime_is_numeric=True).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rp79L8Q04IW0"
   },
   "source": [
    "**Instrumentalness and Acousticness Classification retreatment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ9r8J4J4fd1"
   },
   "source": [
    "Acousticness has been identified as a major correlation for the popularity and represent unbalanced data so we must investigate on it and see if we can improve the data. Also as seen previously intrumentalness and acousticness data do not have any homegeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:06.329654Z",
     "start_time": "2021-03-31T10:02:06.208271Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "jYMI8E13ybAP",
    "outputId": "e1b6d69e-4047-4f61-ac1d-17422ec7f08d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.distplot(df_raw['acousticness'], kde=False, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:09.469461Z",
     "start_time": "2021-03-31T10:02:09.362789Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "cL7WxugdybAP",
    "outputId": "2954b263-4f28-43fb-cc0a-22a3b52c07a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.distplot(df_raw['instrumentalness'], kde=False, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ooYpjc_ybAQ"
   },
   "source": [
    "Songs seem to be either acoustic or not, they are likely not a combination of both. We will then create categories (acoustic, mix, non-acoustic). Similarly, we'll do the same thing on the instrumentalness and loudness feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:11.736379Z",
     "start_time": "2021-03-31T10:02:11.636479Z"
    },
    "id": "uM1IULp-ybAQ"
   },
   "outputs": [],
   "source": [
    "df_raw['instrumentalness'] = df_raw['instrumentalness'].map((lambda x: 1 if x < 0.1 else (3 if x > 0.8 else 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:12.403452Z",
     "start_time": "2021-03-31T10:02:12.317111Z"
    },
    "id": "ChP7ZaEtybAQ"
   },
   "outputs": [],
   "source": [
    "df_raw['acousticness'] = df_raw['acousticness'].map((lambda x: 1 if x < 0.1 else (3 if x > 0.9 else 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "EWi9f3bHBd4g",
    "outputId": "d8216030-2151-40dd-8ef7-9db911fd4e39"
   },
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-DYVKvW4gWx"
   },
   "source": [
    "## C) Evaluation of the date information within our Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUS1nmkrG57W"
   },
   "outputs": [],
   "source": [
    "#df_raw['year'] = pd.DatetimeIndex(df_raw['release_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:14.241804Z",
     "start_time": "2021-03-31T10:02:14.077131Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "u0KhWqa-ybAQ",
    "outputId": "7add3053-c93d-479d-a2f1-dafc7fba85af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "ax = df_raw.groupby('year')['popularity'].mean().plot()\n",
    "ax.set_title('Mean Popularity per year', weight='bold')\n",
    "ax.set_ylabel('Mean Popularity')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_xticks(range(1920, 2021, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5X53GvsybAQ"
   },
   "source": [
    "We will wipe out the 2021 song because there is not enough popularity data on this year and so to avoid any outlier influence we keep only the rest of the dataset. We do the same for the song before 1955 as their popularity data appears not relevant. ( Their low popularity could be explained by the lack of diffusion on these time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:19.338956Z",
     "start_time": "2021-03-31T10:02:19.253282Z"
    },
    "id": "w0v8I1C8ybAR"
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw[df_raw['year'] < 2021]\n",
    "df_raw = df_raw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NaCpKW97NZB"
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw[df_raw['year'] > 1955]\n",
    "df_raw = df_raw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "bs4tIKUeG5Up",
    "outputId": "5072241b-3676-42a7-f184-b4872ec756f8"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "ax = df_raw.groupby('year')['popularity'].mean().plot()\n",
    "ax.set_title('Mean Popularity per year', weight='bold')\n",
    "ax.set_ylabel('Mean Popularity')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_xticks(range(1956, 2020, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2NNBQ4ZH8WE"
   },
   "source": [
    "It could also be interesting to investigate on the seasonality of the popularity. Let's create a column with the month and see if song are more popular among a season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMyK_HeRIffs"
   },
   "outputs": [],
   "source": [
    "df_raw['month'] = pd.DatetimeIndex(df_raw['release_date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "ORBhskmAI9H3",
    "outputId": "961774ac-61bc-4b9d-8f41-b7fd02165242"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"month\", y=\"popularity\", data=df_raw, alpha=0.01, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6_ufxeiJNLD"
   },
   "source": [
    "It seems that there is no correlation between the month and the popularity level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwg43IMwybAR"
   },
   "source": [
    "<a id=\"output_variable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5I4loDk59wlr"
   },
   "source": [
    "# **D) Convert MS duration into minute duration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPCXzODgAdDF"
   },
   "source": [
    "In the current dataset all the music times are displayed within ms, which could badly influence our model so we choose to convert it into minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvTSwyh994DK"
   },
   "outputs": [],
   "source": [
    "df_raw[\"duration_min\"] = df_raw[\"duration_ms\"]/(1000 * 60)\n",
    "df_raw[\"duration_min\"] = df_raw[\"duration_min\"].astype(int)\n",
    "df_raw = df_raw.drop(labels=['duration_ms'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "SIaSS8dIz5wL",
    "outputId": "71a91bf7-7217-41ca-e30c-22ce36e8d2a3"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"duration_min\", y=\"popularity\", data=df_raw, alpha=0.03, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW93w-uS0UVD"
   },
   "source": [
    "The above graph show us that the popularity appears correlate with the duration. Indeed, the longer songs seems to be less popular ( >15min), also short songs seems to be less popular ( <2min30 ). So we will create three different classification to represent the duration differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ87csEC0VGB"
   },
   "outputs": [],
   "source": [
    "df_raw['duration_min'] = df_raw['duration_min'].map((lambda x: 1 if x < 2.30 else (3 if x > 15 else 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCpAuLS687y7"
   },
   "source": [
    "# **E) Language of the song**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwNP1Pbd9Hf3"
   },
   "source": [
    "We suppose that the song language influence its popularity. Indeed, our dataset include song from all over the world and a Russian song will obviously be less popular than an English song that could be more oftenly become a worldwide hit. \n",
    "Therefor we will try to identify the language of everysong from the music name and see if it correlates with the popularity. If that the case, we will keep this data to improve the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b02rVbxD9oSK",
    "outputId": "abca487f-7626-40b8-e2ed-7220bc341df5"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect\n",
    "import langdetect\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrbMH--ES3S2",
    "outputId": "9481fbe5-c7d9-4a42-b0da-68866babb36e"
   },
   "outputs": [],
   "source": [
    "for index, row in df_raw['name'].iteritems():\n",
    "  try:\n",
    "    lang = detect(str(row)) #detecting each row\n",
    "    df_raw.loc[index, 'language'] = lang\n",
    "    print(index)\n",
    "  except:\n",
    "    lang = 'na'\n",
    "    df_raw.loc[index, 'language'] = lang\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ8OgnnMrPd1",
    "outputId": "67fd7bab-7404-4159-d6e4-da1238e32f9f"
   },
   "outputs": [],
   "source": [
    "#df_raw['languageindex'] = pd.DataFrame(df_raw, columns= ['language'])\n",
    "\n",
    "nombrelangue = df_raw.pivot_table(index=['language'], aggfunc='size')\n",
    "print(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZEccKWvvhIG"
   },
   "source": [
    "Now we have for each country an unique value that can be use within our model later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2vMCUwhBlnh",
    "outputId": "6518ba7c-cab6-4cea-ece2-407d8f287b9f"
   },
   "outputs": [],
   "source": [
    "corr = np.abs(df_raw.corr())\n",
    "series = np.abs(corr['popularity']).sort_values(ascending=False)\n",
    "for i, row in enumerate(series):\n",
    "    if 0.1 <= row < 1:\n",
    "      print(f'{series.index[i]:17} --> {row: .2f} (abs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZjoT_RBNCmA"
   },
   "source": [
    "After the preprocessing we observ that the correlation between the popularity and the data set have changed. The Artist popularity score and the year appears now as the main correlators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxLM7Q5Ix4-0"
   },
   "source": [
    "# **F) Dropping of Useless Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "1Jv55j2d2qpT",
    "outputId": "a59c753a-4c73-4931-f52b-8d18877cc378",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGGHkP-Jyf_9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw.drop(labels=['id','name','year','artist2','artist3','artist4','artist5','main_artist'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"convert_date\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTe_MwOfybAS"
   },
   "source": [
    "# $\\bullet$ First  Model \n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:43.613842Z",
     "start_time": "2021-03-31T10:02:43.556070Z"
    },
    "id": "X7taa2mlybAS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_backup = copy.deepcopy(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:44.111500Z",
     "start_time": "2021-03-31T10:02:44.089121Z"
    },
    "id": "8CuJaTgYybAS"
   },
   "outputs": [],
   "source": [
    "df_raw = df_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:45.094245Z",
     "start_time": "2021-03-31T10:02:44.948706Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2rt4uanybAT",
    "outputId": "af121972-c48f-49d6-a3c9-c3d58495a293"
   },
   "outputs": [],
   "source": [
    "add_datepart(df_raw, 'release_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:51.134972Z",
     "start_time": "2021-03-31T10:02:50.929260Z"
    },
    "id": "MCSZ0MiVybAT"
   },
   "outputs": [],
   "source": [
    "\n",
    "df, y, nas = proc_df(df_raw, 'popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:52.156183Z",
     "start_time": "2021-03-31T10:02:52.109613Z"
    },
    "id": "4Oyx3UlgybAT"
   },
   "outputs": [],
   "source": [
    "def rmse(y_gold, y_pred): \n",
    "    return math.sqrt(((y_gold - y_pred)**2).mean())\n",
    "\n",
    "def print_score(m, X_train, y_train, X_valid, y_valid):\n",
    "    print('RMSE on train set: {:.4f}'.format(rmse(m.predict(X_train), y_train)))\n",
    "    print('RMSE on valid set: {:.4f}'.format(rmse(m.predict(X_valid), y_valid)))\n",
    "    print('R^2 on train set: {:.4f}'.format(m.score(X_train, y_train)))\n",
    "    print('R^2 on valid set: {:.4f}'.format(m.score(X_valid, y_valid)))\n",
    "    if hasattr(m, 'oob_score_'): print('R^2 on oob set: {:.4f}'.format(m.oob_score_))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:02:53.652913Z",
     "start_time": "2021-03-31T10:02:53.610605Z"
    },
    "id": "nRxUfodKybAU"
   },
   "outputs": [],
   "source": [
    "def split_vals(df, n): \n",
    "    return df[:n].copy(), df[n:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:10:12.396281Z",
     "start_time": "2021-03-31T10:10:12.316437Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_qpxHc9ybAU",
    "outputId": "da30f19c-f71b-407b-875e-42ca27b22328"
   },
   "outputs": [],
   "source": [
    "n_total = len(df)\n",
    "n_valid = 25000\n",
    "n_train = n_total - n_valid\n",
    "n_small = 25000\n",
    "\n",
    "print('full number of data points : {}'.format(n_total))\n",
    "print('number of validation data points : {}'.format(n_valid))\n",
    "print('number of training data points : {}'.format(n_train))\n",
    "print('number of subsampled training points : {}'.format(n_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:11:35.985509Z",
     "start_time": "2021-03-31T10:11:35.940584Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g4P3mGloybAU",
    "outputId": "849dad3f-61e1-41ec-f006-8dd290d7e051"
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw.sample(frac=1).reset_index(drop=True)\n",
    "df, y, nas = proc_df(df_raw, 'popularity')\n",
    "\n",
    "X_train, X_valid = split_vals(df, n_train)\n",
    "y_train, y_valid = split_vals(y, n_train)\n",
    "X_small, _ = split_vals(X_train, n_small)\n",
    "y_small, _ = split_vals(y_train, n_small)\n",
    "\n",
    "print('Number of full training data points: X = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
    "print('Number of small training data points: X = {}, y = {}'.format(X_small.shape, y_small.shape))\n",
    "print('Number of validation data points: X = {}, y = {}'.format(X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:13:16.216202Z",
     "start_time": "2021-03-31T10:12:38.067246Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCSaa9bPybAV",
    "outputId": "a628c446-cc65-4f88-f514-5d569b80e1d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = RandomForestRegressor(n_jobs = -1, random_state = 42)\n",
    "\n",
    "%time base_model.fit(X_train, y_train)\n",
    "print_score(base_model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9rKpN4JybAV"
   },
   "source": [
    "<a id=\"convert_date\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "CfkT54i6ybAV"
   },
   "source": [
    "### $\\bullet$ Save preprocessed data\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "O-QnX9sNybAV"
   },
   "source": [
    "But let's save this file for now, since it's already in a format which can be stored and accessed efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:43:57.507517Z",
     "start_time": "2021-03-11T08:43:57.466117Z"
    },
    "hidden": true,
    "id": "hde4ILY-ybAW"
   },
   "outputs": [],
   "source": [
    "path_to_tmp = os.path.join(path_to_repo, \"data\", \"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:43:58.058373Z",
     "start_time": "2021-03-11T08:43:58.022497Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "E8f6cnSSybAW",
    "outputId": "cde2701b-c758-45c8-f44d-08a5d87bf037"
   },
   "outputs": [],
   "source": [
    "path_to_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:43:58.617673Z",
     "start_time": "2021-03-11T08:43:58.584219Z"
    },
    "hidden": true,
    "id": "zH2fOSGYybAW"
   },
   "outputs": [],
   "source": [
    "os.makedirs(path_to_tmp, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:43:59.466124Z",
     "start_time": "2021-03-11T08:43:59.425162Z"
    },
    "hidden": true,
    "id": "t7PnSKbbybAW"
   },
   "outputs": [],
   "source": [
    "path_to_data_raw = os.path.join(path_to_tmp, 'spotify-raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T08:44:00.419921Z",
     "start_time": "2021-03-11T08:44:00.377201Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "7Yx4Or9kybAW",
    "outputId": "f5c5b241-b99a-44ae-d531-2fabf647d6a6"
   },
   "outputs": [],
   "source": [
    "path_to_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T15:38:36.222194Z",
     "start_time": "2021-03-09T15:38:35.820620Z"
    },
    "hidden": true,
    "id": "PGwGCm0nybAW"
   },
   "outputs": [],
   "source": [
    "df.to_feather(path_to_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-09T15:38:38.122332Z",
     "start_time": "2021-03-09T15:38:36.574839Z"
    },
    "hidden": true,
    "id": "vbhEXSQnybAX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_to_data_raw + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "FS5LP2XmybAX"
   },
   "source": [
    "<a id=\"numericalize_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82bshpPAybAX"
   },
   "source": [
    "# 3. Random Forests\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA9wuv-TybAf"
   },
   "source": [
    "### $\\bullet$ Out-of-bag (OOB) score\n",
    "\n",
    "The OOB score is a good indicator to have a clearer view of how the model performs.\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:21:31.146012Z",
     "start_time": "2021-03-31T10:21:27.358160Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZaKWQ6jybAf",
    "outputId": "3cb6361c-72d3-4cec-f712-10856802e82c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_jobs = -1, \n",
    "    random_state = 42,\n",
    "    \n",
    "    bootstrap = True,\n",
    "    oob_score = True, # default = False, \n",
    "    max_samples = None,\n",
    ")\n",
    "\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:42.427721Z",
     "start_time": "2021-02-10T15:06:08.864305Z"
    },
    "id": "1WRFsBeAybAf"
   },
   "source": [
    "The model is slightly overfitting, we should explore solutions to prevent that from happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB7y1YPkybAf"
   },
   "source": [
    "<a id='data_sampling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZhkqnXIybAf"
   },
   "source": [
    "### $\\bullet$ Data subsampling\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEhlaL5DybAf"
   },
   "source": [
    "It turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: *subsampling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:04:06.046980Z",
     "start_time": "2021-03-17T12:03:49.519133Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeQ3sJm-ybAg",
    "outputId": "d3b032a6-f3ac-4184-b5f2-08b47a4d56a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_jobs = -1, \n",
    "    random_state = 42,\n",
    "    \n",
    "    bootstrap = True,\n",
    "    oob_score = True, # default = False, \n",
    "    max_samples = 0.15, # default = None,\n",
    ")\n",
    "\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KbK5KAMybAg"
   },
   "source": [
    "<a id='feature_sampling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrssmpryybAg"
   },
   "source": [
    "### $\\bullet$ Feature subsampling\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:07:11.634243Z",
     "start_time": "2021-03-17T12:06:46.693511Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_LKMKjEybAg",
    "outputId": "f581c14b-de1e-43f0-997e-b0a5c7c53868"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    min_samples_split = 10,\n",
    "    max_features = 0.5, # default = 'auto'/None\n",
    "    n_jobs = -1, \n",
    "    random_state = 42,\n",
    "    \n",
    "    bootstrap = True,\n",
    "    oob_score = True, # default = False, \n",
    "    max_samples = 0.75, # default = None,\n",
    ")\n",
    "\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-Bb8nh1ybAh"
   },
   "source": [
    "<a id='general_bagging'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "34fSG-jyybAi"
   },
   "source": [
    "### $\\bullet$ Cross validation\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:46:50.182815Z",
     "start_time": "2021-02-10T15:46:46.124758Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "XxEEUesQybAi",
    "outputId": "e3908e1f-3cf3-4604-e336-186d0fe9382b"
   },
   "outputs": [],
   "source": [
    "# native Random Forest\n",
    "model = RandomForestRegressor(\n",
    "    min_samples_split = 2,\n",
    "    max_features = None, # default = 'auto'/None\n",
    "    n_jobs = -1, \n",
    "    random_state = 42,\n",
    "    \n",
    "    bootstrap = True,\n",
    "    oob_score = True, # default = False, \n",
    "    max_samples = None,\n",
    ")\n",
    "\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:47:13.473051Z",
     "start_time": "2021-02-10T15:46:51.685690Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "UUpdZkMiybAi",
    "outputId": "73e103c5-83b8-43b5-f8ed-1a5d36c068a2"
   },
   "outputs": [],
   "source": [
    "# default is cv = 5\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_small, y_small, cv = 8, n_jobs = -1)\n",
    "\n",
    "print(scores)\n",
    "print(\"r2 is {:.3f} with a standard deviation of {:.3f}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "ISUwLU1JybAj"
   },
   "source": [
    "- Change the score formula (ex: switch from $R^2$ to Root Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:47:55.909293Z",
     "start_time": "2021-02-10T15:47:33.907850Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "5aPano8-ybAj",
    "outputId": "f69636f5-a3bd-4eb1-e736-5bc99802ece2"
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_small, y_small, cv = 8, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "print(scores)\n",
    "print(\"r2 is {:.3f} with a standard deviation of {:.3f}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:48:19.405617Z",
     "start_time": "2021-02-10T15:47:55.910606Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Ohh6uZBCybAj",
    "outputId": "e47c51cf-4a77-40dd-8cfe-f8c965a18556"
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_small, y_small, cv = 8, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n",
    "\n",
    "print(scores)\n",
    "print(\"RMSE is {:.3f} with a standard deviation of {:.3f}\".format(-scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:48:19.442893Z",
     "start_time": "2021-02-10T15:48:19.407168Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "9_mFfbTkybAj",
    "outputId": "76d636cc-ffa5-4f15-8b3d-3c526b2daec9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of possible keys for scoring\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "LxxI1cZKybAj"
   },
   "source": [
    "<a id='hyperparameter_tuning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCCcIXz_ybAk"
   },
   "source": [
    "### $\\bullet$ Hyperparameter tuning for final model selection\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:24:12.899040Z",
     "start_time": "2021-03-31T10:24:07.332365Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFyoLSbKybAk",
    "outputId": "236a9cba-581a-4a0e-efa6-9f1f95fe798b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_jobs = -1, \n",
    "    random_state = 42,\n",
    "    \n",
    "    bootstrap = True,\n",
    "    oob_score = True, # default = False, \n",
    "    max_samples = None,\n",
    ")\n",
    "\n",
    "%time model.fit(X_small, y_small)\n",
    "print_score(model, X_small, y_small, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:26:32.525378Z",
     "start_time": "2021-03-31T10:26:32.460320Z"
    },
    "id": "xRGKulfVybAk"
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    \n",
    "    {'n_estimators': [30],\n",
    "     'max_depth': [5,10,15],\n",
    "     'min_samples_split': [2,5,10,15],\n",
    "     'min_samples_leaf':[1,5,10]\n",
    "    },\n",
    " ]\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    random_state = 42,\n",
    "    bootstrap = True,\n",
    "    oob_score = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:26:34.933529Z",
     "start_time": "2021-03-31T10:26:34.890698Z"
    },
    "id": "ultTRus2ybAk"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_model = GridSearchCV(\n",
    "    estimator = model, \n",
    "    param_grid = param_grid, \n",
    "    scoring = None, # uses estimator's default score method\n",
    "    n_jobs = -1, \n",
    "    refit = True, # keep a fitted version of the overall best model\n",
    "    cv = 5, \n",
    "    return_train_score = True,\n",
    "    verbose = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:27:24.520601Z",
     "start_time": "2021-03-31T10:26:35.548727Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AWaLqYCybAk",
    "outputId": "daef7510-7b5a-4af8-d0fb-7fba82da3aaf"
   },
   "outputs": [],
   "source": [
    "%time tuned_model.fit(X_small, y_small)\n",
    "\n",
    "print('Train R^2 Score : %.3f'%tuned_model.best_estimator_.score(X_train, y_train))\n",
    "print('Test R^2 Score : %.3f'%tuned_model.best_estimator_.score(X_valid, y_valid))\n",
    "print('Best R^2 Score Through Grid Search : %.3f'%tuned_model.best_score_)\n",
    "print('Best Parameters : ',tuned_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:27:25.192065Z",
     "start_time": "2021-03-31T10:27:25.173296Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BTb0EYtybAl",
    "outputId": "9d0dad59-b507-432e-f710-90a01a6b661b"
   },
   "outputs": [],
   "source": [
    "best_model = tuned_model.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:27:25.962376Z",
     "start_time": "2021-03-31T10:27:25.731424Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShGTLmeKybAl",
    "outputId": "9d3195a4-8d9d-4fd4-b598-df38c5f4d030",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_score(best_model, X_small, y_small, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model doesn't seem to be overfitting, the scores ares a bit worse that what we could find while we were exploring solutions. Let's check another parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:29:17.607569Z",
     "start_time": "2021-03-31T10:29:17.491305Z"
    },
    "id": "g1zawh8UybAl"
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [30],\n",
    "     'max_depth': [10, 20, None],\n",
    "     'min_samples_split': [2, 10, 50],\n",
    "     'max_features': [0.25, 0.5, 0.75],\n",
    "     'max_samples': [0.2, 0.5, 0.75],\n",
    "    },\n",
    " ]\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    random_state = 42,\n",
    "    oob_score = True,\n",
    "    bootstrap = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:29:18.720596Z",
     "start_time": "2021-03-31T10:29:18.702177Z"
    },
    "id": "k08yc-2ZybAl"
   },
   "outputs": [],
   "source": [
    "tuned_model = GridSearchCV(\n",
    "    estimator = model, \n",
    "    param_grid = param_grid, \n",
    "    scoring = None, # uses estimator's default score method\n",
    "    n_jobs = -1, \n",
    "    refit = True, # keep a fitted version of the overall best model\n",
    "    cv = 5, \n",
    "    return_train_score = True,\n",
    "    verbose = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:29:49.786410Z",
     "start_time": "2021-03-31T10:29:19.230455Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4pndE4JybAl",
    "outputId": "88a6556c-ad83-468c-87b6-336144c8fe05"
   },
   "outputs": [],
   "source": [
    "%time tuned_model.fit(X_small, y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:29:50.378555Z",
     "start_time": "2021-03-31T10:29:50.357961Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tg-EPTSybAm",
    "outputId": "611d8232-7553-4c17-8d87-0e8bdb1b373d"
   },
   "outputs": [],
   "source": [
    "best_model = tuned_model.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:29:51.144296Z",
     "start_time": "2021-03-31T10:29:50.887018Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_waCK-VlybAm",
    "outputId": "00be419b-6cdd-4595-f2b4-9029ce0a848e"
   },
   "outputs": [],
   "source": [
    "print_score(best_model, X_small, y_small, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:32:38.750818Z",
     "start_time": "2021-03-31T10:32:36.922206Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "JlpB3c74ybAm",
    "outputId": "35665829-4a30-4de2-c71f-e84396364020"
   },
   "outputs": [],
   "source": [
    "# randomized splitting strategy\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "scores = cross_val_score(best_model, X_small, y_small, cv = cv, n_jobs = -1)\n",
    "\n",
    "print(scores)\n",
    "print(\"r2 is {:.3f} with a standard deviation of {:.3f}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:36:43.436075Z",
     "start_time": "2021-03-31T10:36:19.001673Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfTh8HeTybAm",
    "outputId": "6bb27f78-d435-489e-ed9a-e2dd8f7e65d0"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_depth=10, max_features=0.75, max_samples=0.75,\n",
    "                      n_estimators=30, oob_score=True, random_state=42\n",
    ")\n",
    "\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68FyH14kybAn"
   },
   "source": [
    "# 4. Classification, and differences with Regression\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv0OkoeYybAn"
   },
   "source": [
    "We shall change our target variable from log sale prices to a set of _categorical sale prices_. Then we will use the `RandomForestClassifier` model to compute prediction. Classifiers are typically used in order to predict one class among a finite set of possibilities, however these models actually generate a *probability estimate* over the full set of possibilities. The resulting prediction is then simply taken as the most likely class given this output probability estimate.\n",
    "\n",
    "\n",
    "Let us introduce the use of classifiers on out dataset, with the continuous variable *SalePrice* converted into a categorical one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdVsTSHbybAn"
   },
   "source": [
    "<a id='categorical_target'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSt5NPboybAn"
   },
   "source": [
    "### $\\bullet$ Categorical target\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "We shall change our target variable from log sale prices to a set of _categorical sale prices_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:57:00.475735Z",
     "start_time": "2021-03-31T10:56:59.152661Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "70uQ5J6QybAn",
    "outputId": "f09398e8-f13d-4f4e-bea3-643579d8d9f8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.Series(y_train).plot(kind = 'density', bw_method = 0.05);\n",
    "\n",
    "# alternatively\n",
    "# sns.set_style('whitegrid')\n",
    "# sns.kdeplot(np.array(y_train), bw_method = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8TV9n0qybAn"
   },
   "source": [
    "We will recast the sale Price prediction problem as a _binary classification_ problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:57:29.227195Z",
     "start_time": "2021-03-31T10:57:29.178662Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cjXfpheybAo",
    "outputId": "71c217fd-1701-4b92-c285-a5c6097755ab"
   },
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:57:54.135659Z",
     "start_time": "2021-03-31T10:57:54.074230Z"
    },
    "id": "aAych8uaybAo"
   },
   "outputs": [],
   "source": [
    "# intervals are taken along the median (10.165):\n",
    "# ]8.463, 10.165]\n",
    "# ]10.165, 11.864]\n",
    "intervals_2 = pd.cut(y, bins = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:57:54.936213Z",
     "start_time": "2021-03-31T10:57:54.914394Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ABlUaCtybAo",
    "outputId": "3baf26da-22d9-4dab-d1c6-93fe8fc834b2"
   },
   "outputs": [],
   "source": [
    "intervals_2[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:57:58.749327Z",
     "start_time": "2021-03-31T10:57:58.706482Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWdTNOrxybAo",
    "outputId": "1c571655-e578-4a85-bd73-e44b6824d2e3"
   },
   "outputs": [],
   "source": [
    "type(intervals_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:58:02.049380Z",
     "start_time": "2021-03-31T10:58:01.430566Z"
    },
    "id": "qaO5A6qaybAo"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_cat_2 = label_encoder.fit_transform(intervals_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:58:02.584710Z",
     "start_time": "2021-03-31T10:58:02.567395Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QATQHYoOybAo",
    "outputId": "64cec7f3-9772-4355-beeb-8ab4f67ad511"
   },
   "outputs": [],
   "source": [
    "y_cat_2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T10:58:12.553791Z",
     "start_time": "2021-03-31T10:58:12.508817Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJOKNLfDybAp",
    "outputId": "c18024a9-8b2a-41c3-cabe-892d8dbe1864"
   },
   "outputs": [],
   "source": [
    "label_encoder.classes_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T11:08:34.281773Z",
     "start_time": "2021-03-31T11:08:34.234681Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Hj_sm3_ybAp",
    "outputId": "9d7d3584-ba75-42ef-c6a0-4e2ebe69b55c"
   },
   "outputs": [],
   "source": [
    "y_cat_2_train, y_cat_2_valid = split_vals(y_cat_2, n_train)\n",
    "y_cat_2_small, _ = split_vals(y_cat_2_train, n_small)\n",
    "\n",
    "y_cat_2_train.shape, y_cat_2_valid.shape, y_cat_2_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T11:09:21.717226Z",
     "start_time": "2021-03-31T11:09:20.417886Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "heOT3MVrybAp",
    "outputId": "ef3b113f-c011-437a-ece5-0a4ba4d878ba"
   },
   "outputs": [],
   "source": [
    "pd.Series(y_cat_2_train).plot(kind = 'density', bw_method = 0.05);\n",
    "\n",
    "# alternatively\n",
    "# sns.set_style('whitegrid')\n",
    "# sns.kdeplot(np.array(y_train), bw_method = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FCr-0SqybAp"
   },
   "source": [
    "<a id='random_forest_classifier'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRJ1PW4LybA1"
   },
   "source": [
    "The calibrated version of the Random Forest Classifier has more balanced precision and recall scores. These scores are moreover more stable with respect to the chosen threshold, as shifting the classification threshold from 0.4 to 0.6 induces this time a reduced 9% precision jump and 18% recall jump.\n",
    "\n",
    "#### Calibration for probability interpretation as confidence levels\n",
    "\n",
    "We use here calibration as a mean to reduce variance in the resulting deterministic classifiers when shifting the decision threshold. Originally, this method was designed in order to get probabilistic models whose _probability estimates truly correspond to confidence levels_, in the sense that the region of input points that were assigned a probability of $p$ to be of a given label should form a region where the empirical ratio of points with this label is $p$.<br>\n",
    "For more details see the [scikit-learn documentation](https://scikit-learn.org/stable/modules/calibration.html#calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5rDXNp2ybA1"
   },
   "source": [
    "### $\\bullet$ Multiclass classification\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "Let us replicate the process followed for binary classification to the case of 5-fold classification\n",
    "\n",
    "#### Data categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_classifier = CalibratedClassifierCV(\n",
    "    base_estimator = classifier,\n",
    "    method = 'sigmoid', # 'sigmoid' or 'isotonic'\n",
    "    ensemble = True,\n",
    "    cv = 8, # int of 'prefit'\n",
    "    n_jobs = -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T11:45:46.261848Z",
     "start_time": "2021-03-31T11:45:44.924141Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "TozbhLvkybA1",
    "outputId": "0014b71d-e4b7-402a-829c-08b7220c0f03"
   },
   "outputs": [],
   "source": [
    "probas = calibrated_classifier.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "precision_recall_curve_with_threshold(y_cat_2_valid, probas, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T11:45:57.127819Z",
     "start_time": "2021-03-31T11:45:57.069882Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drzgF-4_ybA1",
    "outputId": "a1205cc9-8581-4c4d-e8d8-70fbacbb39b2"
   },
   "outputs": [],
   "source": [
    "intervals_5 = pd.cut(y, bins = 5)\n",
    "intervals_5[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T11:46:03.950798Z",
     "start_time": "2021-03-31T11:46:03.123531Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85s8NiZnybA2",
    "outputId": "e879d096-c747-4221-f5a8-6edc6e4c2b08"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_cat_5 = label_encoder.fit_transform(intervals_5)\n",
    "y_cat_5[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24fmDDCLybA2",
    "outputId": "08ba0be7-fe31-44d9-abdd-1427cd2fb0c8"
   },
   "outputs": [],
   "source": [
    "label_encoder.classes_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRdGygkdybA2",
    "outputId": "6578016c-860d-4bbb-cab6-a15aec6f59fb"
   },
   "outputs": [],
   "source": [
    "y_cat_5_train, y_cat_5_valid = split_vals(y_cat_5, n_train)\n",
    "y_cat_5_small, _ = split_vals(y_cat_5, n_small)\n",
    "\n",
    "y_cat_5_train.shape, y_cat_5_valid.shape, y_cat_5_small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdiuah8jybA2"
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "qqTt2qFEybA2",
    "outputId": "b6b03e39-5a57-425c-ffb4-e7f92a593540"
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(\n",
    "    n_estimators = 20, \n",
    "    class_weight = 'balanced', # classifier specific\n",
    "    criterion = 'gini',  # classifier specific\n",
    "    max_depth = 3, \n",
    "    min_samples_split = 2, \n",
    "    min_samples_leaf = 1, \n",
    "    min_weight_fraction_leaf = 0.0, \n",
    "    max_features = 'auto', \n",
    "    max_leaf_nodes = None, \n",
    "    min_impurity_decrease = 0.0, \n",
    "    min_impurity_split = None, \n",
    "    ccp_alpha = 0.0, \n",
    "    random_state = 42, \n",
    "    bootstrap = True, \n",
    "    oob_score = True, \n",
    "    max_samples = None,\n",
    "    warm_start = False, \n",
    "    n_jobs = -1, \n",
    "    verbose = 0, \n",
    ")\n",
    "\n",
    "\n",
    "calibrated_classifier = CalibratedClassifierCV(\n",
    "    base_estimator = classifier,\n",
    "    method = 'sigmoid', # 'sigmoid' or 'isotonic'\n",
    "    ensemble = True,\n",
    "    cv = 5, # int of 'prefit'\n",
    "    n_jobs = -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6ncIm0LybA3"
   },
   "outputs": [],
   "source": [
    "%time classifier.fit(X_small, y_cat_5_small)\n",
    "\n",
    "print(classifier.score(X_small, y_cat_5_small)) # classification accuracy on training set\n",
    "print(classifier.score(X_valid, y_cat_5_valid)) # classification accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zwbkaqhybA3"
   },
   "outputs": [],
   "source": [
    "%time calibrated_classifier.fit(X_small, y_cat_5_small)\n",
    "\n",
    "print(calibrated_classifier.score(X_small, y_cat_5_small)) # classification accuracy on training set\n",
    "print(calibrated_classifier.score(X_valid, y_cat_5_valid)) # classification accuracy on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOgJutQCybA3"
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "We remind that intuitively, precision is the ability of a binary classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "Evaluation of a multiclass classifier can, in addition to computing the overall accuracy, either be computed using  `macro` average or `weighted` macro-average of precision, recall and F1-score :\n",
    "\n",
    "- macro : the macro-average of the recall score is obtained by taking the average, over all classes $c$, of the ratio of well-classified points among points with true class $c$, whereas the precision score is obtained by taking the average, over all classes $c$, of the ratio of well-classified points among points with predicted class $c$. The macro-averaged F1-score is taken by applying the F1-score formula to the macro-averages of precision and recall scores.\n",
    "\n",
    "\n",
    "- weighted macro-average : Same as macro, but averages over classes are ponderated by the ratio of points belonging to each class. This permits to take into account the dataset imbalance over the different classes : scores computed over well-represented classes have more weight on the resulting scores, whereas poorly-represented classes have little impact on the resulting scores. This gives a more realistic evaluation since future point sets to classify will typically follow the same imbalance as observed during evaluation.\n",
    "\n",
    "\n",
    "For _multi-label_ classification there is also the notion of `micro` average as alternative to macro and weighted-macro averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTaVTlpHybA4"
   },
   "source": [
    "Weighted macro-averages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ft-bUtFybA5"
   },
   "outputs": [],
   "source": [
    "y_predict_valid = classifier.predict(X_valid)\n",
    "\n",
    "average = 'weighted'\n",
    "\n",
    "acc = metrics.accuracy_score(y_cat_5_valid, y_predict_valid)\n",
    "rec = metrics.recall_score(y_cat_5_valid, y_predict_valid, average = average)\n",
    "prc = metrics.precision_score(y_cat_5_valid, y_predict_valid, average = average)\n",
    "f1  = metrics.f1_score(y_cat_5_valid, y_predict_valid, average = average)\n",
    "\n",
    "print('Accuracy : {:.2f}%'.format(acc*100))\n",
    "print('Recall : {:.2f}%'.format(rec*100))\n",
    "print('Precision : {:.2f}%'.format(prc*100))\n",
    "print('F1-score : {:.2f}%'.format(f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQUvfHTNybA5"
   },
   "outputs": [],
   "source": [
    "y_predict_valid = calibrated_classifier.predict(X_valid)\n",
    "\n",
    "average = 'weighted'\n",
    "\n",
    "acc = metrics.accuracy_score(y_cat_5_valid, y_predict_valid)\n",
    "rec = metrics.recall_score(y_cat_5_valid, y_predict_valid, average = average)\n",
    "prc = metrics.precision_score(y_cat_5_valid, y_predict_valid, average = average)\n",
    "f1  = metrics.f1_score(y_cat_5_valid, y_predict_valid, average = average)\n",
    "\n",
    "print('Accuracy : {:.2f}%'.format(acc*100))\n",
    "print('Recall : {:.2f}%'.format(rec*100))\n",
    "print('Precision : {:.2f}%'.format(prc*100))\n",
    "print('F1-score : {:.2f}%'.format(f1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpFY-v3pybA5"
   },
   "source": [
    "The different scores are better summarized in the following report :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDAP3nCIybA5"
   },
   "outputs": [],
   "source": [
    "y_predict_valid = classifier.predict(X_valid)\n",
    "\n",
    "report = metrics.classification_report(\n",
    "    y_cat_5_valid, \n",
    "    y_predict_valid, \n",
    "    target_names = [str(interval) for interval in label_encoder.classes_.tolist()],\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7jBvZ9YybA5"
   },
   "outputs": [],
   "source": [
    "y_predict_valid = calibrated_classifier.predict(X_valid)\n",
    "\n",
    "report = metrics.classification_report(\n",
    "    y_cat_5_valid, \n",
    "    y_predict_valid, \n",
    "    target_names = [str(interval) for interval in label_encoder.classes_.tolist()],\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0b9j93-ybA6"
   },
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "Ultimately, classification preformances are better understood though the _confusion matrix_ of the model on the evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "id": "xd1RziP6ybA6",
    "outputId": "5ec60083-3722-409e-de5e-e32dcaf8ed73"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "metrics.plot_confusion_matrix(\n",
    "    classifier, \n",
    "    X_valid, \n",
    "    y_cat_5_valid, \n",
    "    display_labels = [str(interval) for interval in label_encoder.classes_.tolist()],\n",
    "    include_values = True, \n",
    "    xticks_rotation = 45, \n",
    "    values_format = None, \n",
    "    cmap = 'viridis', \n",
    "    ax = ax,\n",
    "    colorbar = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "PBHsq6QCybA6",
    "outputId": "b1b8fd04-bd3d-4edb-debe-ef661938e542",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "metrics.plot_confusion_matrix(\n",
    "    calibrated_classifier, \n",
    "    X_valid, \n",
    "    y_cat_5_valid, \n",
    "    display_labels = [str(interval) for interval in label_encoder.classes_.tolist()],\n",
    "    include_values = True, \n",
    "    xticks_rotation = 45, \n",
    "    values_format = None, \n",
    "    cmap = 'viridis', \n",
    "    ax = ax,\n",
    "    colorbar = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Interpretation\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='RF_feature_importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Random Forest feature importance\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "It's normally not enough to just know that a model can make accurate predictions - we also want to know *how* it's making predictions. The most important way to see this is with *feature importance*.\n",
    "\n",
    "Scikit-learn's implementation of Random Forests carry _impurity-based_ feature importance scores (the higher, the more important the feature) in the attribute `model.feature_importances_` of a fitted model. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature on the training set. \n",
    "\n",
    "Since they are computed on the training set these scores should be taken with caution, as the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit. As a result, features that are deemed of low importance for a bad model (high training score but low validation/cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a validation set (or better with cross-validation) prior to computing importances. These feature importance does not reflect the intrinsic predictive value of a feature by itself, but how important this feature is for a particular random forest.\n",
    "\n",
    "\n",
    "See more on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/partial_dependence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:26:02.985113Z",
     "start_time": "2021-04-22T15:26:02.945605Z"
    }
   },
   "outputs": [],
   "source": [
    "def rf_feat_importance(model, df):\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'cols': df.columns, \n",
    "        'imp': model.feature_importances_,\n",
    "    })\n",
    "    feat_importance = feat_importance.sort_values('imp', ascending = False)\n",
    "    return feat_importance\n",
    "\n",
    "\n",
    "def rf_unfolded_feat_importance(model, df):\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'cols': df.columns, \n",
    "        'imp': model.feature_importances_,\n",
    "    })\n",
    "    for i, tree in enumerate(model.estimators_):\n",
    "        feat_importance['imp_' + str(i)] = tree.feature_importances_\n",
    "    feat_importance = feat_importance.sort_values('imp', ascending = False)\n",
    "    return feat_importance\n",
    "\n",
    "\n",
    "# found at\n",
    "# https://stackoverflow.com/questions/49170296/scikit-learn-feature-importance-calculation-in-decision-trees\n",
    "def tree_feature_importance(model, normalize = True):\n",
    "\n",
    "    left_c = model.tree_.children_left\n",
    "    right_c = model.tree_.children_right\n",
    "\n",
    "    impurity = model.tree_.impurity    \n",
    "    node_samples = model.tree_.weighted_n_node_samples \n",
    "\n",
    "    # Initialize the feature importance, those not used remain zero\n",
    "    feature_importance = np.zeros((model.tree_.n_features,))\n",
    "\n",
    "    for idx, node in enumerate(model.tree_.feature):\n",
    "        if node >= 0:\n",
    "            # Accumulate the feature importance over all the nodes where it's used\n",
    "            feature_importance[node] += (\n",
    "                impurity[idx]*node_samples[idx] -\n",
    "                impurity[left_c[idx]]*node_samples[left_c[idx]] -\n",
    "                impurity[right_c[idx]]*node_samples[right_c[idx]])\n",
    "\n",
    "    # Number of samples at the root node\n",
    "    feature_importance /= node_samples[0]\n",
    "\n",
    "    if normalize:\n",
    "        normalizer = feature_importance.sum()\n",
    "        if normalizer > 0:\n",
    "            feature_importance /= normalizer\n",
    "\n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:36:33.743207Z",
     "start_time": "2021-04-22T15:36:32.126343Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    max_features=0.75, \n",
    "    max_samples=0.75, \n",
    "    max_depth=10,\n",
    "    n_estimators=30, \n",
    "    n_jobs = -1,  \n",
    "    random_state=42,\n",
    "    bootstrap = True,\n",
    "    oob_score = True,\n",
    ")\n",
    "\n",
    "%time model.fit(X_small, y_small)\n",
    "print_score(model, X_small, y_small, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:36:38.099572Z",
     "start_time": "2021-04-22T15:36:37.940899Z"
    }
   },
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance of a single tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:36:40.139214Z",
     "start_time": "2021-04-22T15:36:40.096103Z"
    }
   },
   "outputs": [],
   "source": [
    "# pick the first tree of the forest\n",
    "tree = model.estimators_[0]\n",
    "tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:36:45.059044Z",
     "start_time": "2021-04-22T15:36:45.011449Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_importance = rf_feat_importance(tree, X_small)\n",
    "feat_importance[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:36:53.234024Z",
     "start_time": "2021-04-22T15:36:53.195039Z"
    }
   },
   "outputs": [],
   "source": [
    "df['All_Artists_pop_sum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:37:00.524795Z",
     "start_time": "2021-04-22T15:37:00.291113Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barh = feat_importance[:30].plot('cols', 'imp', 'barh', figsize = (12,10), legend = False)\n",
    "barh = barh.invert_yaxis()\n",
    "barh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total popularity of artists seems to be the most important feature by a lot, followed by temporal features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance of the whole random forest\n",
    "\n",
    "Each tree of the random forest carries its own feature importance along the explanatory variables (variables unseen during training simply have score 0.0). The overall feature importance of the random forest is then naturally defined as the _average, feature by feature, of the importance scores computed by each tree_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:37:04.277716Z",
     "start_time": "2021-04-22T15:37:04.001685Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_importance = rf_feat_importance(model, X_small)\n",
    "feat_importance[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:37:17.914949Z",
     "start_time": "2021-04-22T15:37:17.689690Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "barh = feat_importance[:33].plot('cols', 'imp', 'barh', figsize = (12,10), legend = False)\n",
    "barh = barh.invert_yaxis()\n",
    "barh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the whole forrest, the main columns are still the same (popularity and date), but we can see more musical features at the top of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation of least important features\n",
    "\n",
    "Least important features can be discarded before re-training the random forest, where it yields a slight improvement of the model's validation $R^2$ score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:37:21.444191Z",
     "start_time": "2021-04-22T15:37:21.400889Z"
    }
   },
   "outputs": [],
   "source": [
    "to_keep = feat_importance[feat_importance.imp > 0.005].cols\n",
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:37:22.941405Z",
     "start_time": "2021-04-22T15:37:22.897843Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:37:30.774343Z",
     "start_time": "2021-04-22T15:37:30.651312Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform ablation of columns with low importance\n",
    "df_keep = df[to_keep].copy()\n",
    "\n",
    "n_total = len(df_keep)\n",
    "n_valid = 25000  # same as Kaggle's test set size\n",
    "n_train = n_total - n_valid\n",
    "n_small = 25000\n",
    "\n",
    "X_train_keep, X_valid_keep = split_vals(df_keep, n_train)\n",
    "y_train, y_valid = split_vals(y, n_train)\n",
    "\n",
    "X_small_keep, _ = split_vals(df_keep, n_small)\n",
    "y_small, _ = split_vals(y, n_small)\n",
    "\n",
    "print('Number of small training data points: X = {}, y = {}'.format(X_small.shape, y_small.shape))\n",
    "print('Number of full training data points: X = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
    "print('Number of validation data points: X = {}, y = {}'.format(X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:04.550963Z",
     "start_time": "2021-04-22T15:38:02.995160Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    max_features=0.75, \n",
    "    max_samples=0.75, \n",
    "    max_depth=10,\n",
    "    n_estimators=30, \n",
    "    n_jobs = -1,  \n",
    "    random_state=42,\n",
    "    bootstrap = True,\n",
    "    oob_score = True,\n",
    ")\n",
    "\n",
    "%time model.fit(X_small_keep, y_small)\n",
    "print_score(model, X_small_keep, y_small, X_valid_keep, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:11.709214Z",
     "start_time": "2021-04-22T15:38:11.383998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_importance = rf_feat_importance(model, X_small_keep)\n",
    "\n",
    "barh = feat_importance.plot('cols', 'imp', 'barh', figsize = (12,10), legend = False)\n",
    "barh = barh.invert_yaxis()\n",
    "barh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in fact, very few features are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='permutation_feature_importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Permutation importance\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Permutation feature importance is a model inspection technique that can be used for any fitted estimator, and is not restricted to Random Forests. This is especially useful for non-linear or opaque estimators.\n",
    "\n",
    "\n",
    "The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. Precisely, the `permutation_importance` function takes as input a fitted model and a validation set $\\mathcal{D}$ with $d$ features, and compute the reference $R^2$ score sc of the model on this dataset. It then loops over a predefined number $k$ of repetitions, where during each loop $i$ a set of $d$ copies of the data are created, with in each copy $\\mathcal{D}_{i, j}$ having feature $j$ being randomly shuffled (aka the term _permutation_). The $R^2$ score of the model is then computed on each of these corrupted copies of $\\mathcal{D}$, yielding a set $\\text{sc}_{i, j}$ of scores. The resulting importance score of each feature $j$ is then given by \n",
    "\n",
    "$$ \\text{sc}_j = \\text{sc} - \\frac{1}{k}\\sum_{i}\\text{sc}_{i, j}$$\n",
    "\n",
    "\n",
    "The importance of a feature is then higher as the model score decreases after corrupting this feature.\n",
    " \n",
    "\n",
    "\n",
    "As for Random Forests impurity-based feature importance, feature importance based on permutation is of value only for good models (not overfitted and with descent cross-validated score), and does not reflect to the intrinsic predictive value of a feature by itself, but how important this feature is for a particular model.\n",
    "\n",
    "\n",
    "See more on [scikit-learn documentation](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:21.094080Z",
     "start_time": "2021-04-22T15:38:19.367514Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    max_features=0.75, \n",
    "    max_samples=0.75, \n",
    "    max_depth=10,\n",
    "    n_estimators=30, \n",
    "    n_jobs = -1,  \n",
    "    random_state=42,\n",
    "    bootstrap = True,\n",
    "    oob_score = True,\n",
    ")\n",
    "\n",
    "%time model.fit(X_small, y_small)\n",
    "print_score(model, X_small, y_small, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:22.813613Z",
     "start_time": "2021-04-22T15:38:22.772142Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X_small.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:42.804893Z",
     "start_time": "2021-04-22T15:38:28.540926Z"
    }
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model, \n",
    "    X_valid, \n",
    "    y_valid, \n",
    "    n_repeats = 10,\n",
    "    random_state = 42, n_jobs = -1,\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:43.520684Z",
     "start_time": "2021-04-22T15:38:43.500260Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:38:44.648506Z",
     "start_time": "2021-04-22T15:38:43.967436Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 50))\n",
    "ax.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert = False, \n",
    "    labels = X_valid.columns[sorted_idx],\n",
    ")\n",
    "ax.set_title(\"Permutation Importances (validation set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation of least important features\n",
    "\n",
    "Least important features can be discarded before re-training the random forest, where it yields a **notable improvement** of the model's validation $R^2$ score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:53:58.297328Z",
     "start_time": "2021-04-22T17:53:58.033299Z"
    }
   },
   "outputs": [],
   "source": [
    "bool_filter = result.importances_mean > 0.005\n",
    "bool_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:54:14.756083Z",
     "start_time": "2021-04-22T17:54:14.647395Z"
    }
   },
   "outputs": [],
   "source": [
    "to_keep = df.columns[bool_filter]\n",
    "\n",
    "df_keep = df[to_keep].copy()\n",
    "\n",
    "n_total = len(df_keep)\n",
    "n_valid = 25000  # same as Kaggle's test set size\n",
    "n_train = n_total - n_valid\n",
    "n_small = 25000\n",
    "\n",
    "X_train_keep, X_valid_keep = split_vals(df_keep, n_train)\n",
    "y_train, y_valid = split_vals(y, n_train)\n",
    "\n",
    "X_small_keep, _ = split_vals(df_keep, n_small)\n",
    "y_small, _ = split_vals(y, n_small)\n",
    "\n",
    "print('Number of small training data points: X = {}, y = {}'.format(X_small.shape, y_small.shape))\n",
    "print('Number of full training data points: X = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
    "print('Number of validation data points: X = {}, y = {}'.format(X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:54:20.570425Z",
     "start_time": "2021-04-22T17:54:20.529000Z"
    }
   },
   "outputs": [],
   "source": [
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:54:35.629896Z",
     "start_time": "2021-04-22T17:54:34.127671Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    max_features=0.75, \n",
    "    max_samples=0.75, \n",
    "    max_depth=10,\n",
    "    n_estimators=30, \n",
    "    n_jobs = -1,  \n",
    "    random_state=42,\n",
    "    bootstrap = True,\n",
    "    oob_score = True,\n",
    ")\n",
    "\n",
    "%time model.fit(X_small_keep, y_small)\n",
    "print_score(model, X_small_keep, y_small, X_valid_keep, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:54:46.988456Z",
     "start_time": "2021-04-22T17:54:38.789715Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model, \n",
    "    X_valid_keep, \n",
    "    y_valid, \n",
    "    n_repeats = 10,\n",
    "    random_state = 42, n_jobs = -1,\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# plot boxplot\n",
    "fig, ax = plt.subplots(figsize = (15, 20))\n",
    "ax.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert = False, \n",
    "    labels = X_valid.columns[sorted_idx],\n",
    ")\n",
    "ax.set_title(\"Permutation Importances (validation set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation after training on full data\n",
    "\n",
    "Such ablation may hurt performance when the training (and validation) data is abundent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:10.477178Z",
     "start_time": "2021-04-22T17:55:03.713089Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    max_features=0.75, \n",
    "    max_samples=0.75, \n",
    "    max_depth=10,\n",
    "    n_estimators=30, \n",
    "    n_jobs = -1,  \n",
    "    random_state=42,\n",
    "    bootstrap = True,\n",
    "    oob_score = True,\n",
    ")\n",
    "\n",
    "%time model.fit(X_train, y_train)\n",
    "print_score(model, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:21.394371Z",
     "start_time": "2021-04-22T17:55:13.455810Z"
    }
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model, \n",
    "    X_valid, \n",
    "    y_valid, \n",
    "    n_repeats = 5,\n",
    "    random_state = 42, \n",
    "    n_jobs = -1,\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:22.868611Z",
     "start_time": "2021-04-22T17:55:22.010337Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 50))\n",
    "ax.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert = False, \n",
    "    labels = X_valid.columns[sorted_idx],\n",
    ")\n",
    "ax.set_title(\"Permutation Importances (validation set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:34.862462Z",
     "start_time": "2021-04-22T17:55:34.769762Z"
    }
   },
   "outputs": [],
   "source": [
    "bool_filter = result.importances_mean > 0.005\n",
    "to_keep = df.columns[bool_filter]\n",
    "\n",
    "df_keep = df[to_keep].copy()\n",
    "\n",
    "n_total = len(df_keep)\n",
    "n_valid = 25000  # same as Kaggle's test set size\n",
    "n_train = n_total - n_valid\n",
    "n_small = 25000\n",
    "\n",
    "X_train_keep, X_valid_keep = split_vals(df_keep, n_train)\n",
    "y_train, y_valid = split_vals(y, n_train)\n",
    "\n",
    "X_small_keep, _ = split_vals(df_keep, n_small)\n",
    "y_small, _ = split_vals(y, n_small)\n",
    "\n",
    "print('Number of small training data points: X = {}, y = {}'.format(X_small.shape, y_small.shape))\n",
    "print('Number of full training data points: X = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
    "print('Number of validation data points: X = {}, y = {}'.format(X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:38.522687Z",
     "start_time": "2021-04-22T17:55:38.477966Z"
    }
   },
   "outputs": [],
   "source": [
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:46.820035Z",
     "start_time": "2021-04-22T17:55:41.446463Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    max_features=0.75, \n",
    "    max_samples=0.75, \n",
    "    max_depth=10,\n",
    "    n_estimators=30, \n",
    "    n_jobs = -1,  \n",
    "    random_state=42,\n",
    "    bootstrap = True,\n",
    "    oob_score = True,\n",
    ")\n",
    "\n",
    "%time model.fit(X_train_keep, y_train)\n",
    "print_score(model, X_train_keep, y_train, X_valid_keep, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:50.950606Z",
     "start_time": "2021-04-22T17:55:47.970374Z"
    }
   },
   "outputs": [],
   "source": [
    "# default is cv = 5\n",
    "scores = cross_val_score(model, X_small_keep, y_small, cv = 5, n_jobs = -1)\n",
    "\n",
    "print(scores)\n",
    "print(\"r2 is {:.3f} with a standard deviation of {:.3f}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='redundant_features'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Feature correlations\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "If one computes permutation importance of features when some collinearity occurs, corrupting one feature will have little effect on the models performance because it can get the same information from a correlated feature. Because of that, the permutation importance will show that none of the features are important, which is in contradiction with a high validation that shows that some feature must be important. One approach to handling multicollinearity is by performing hierarchical clustering on the features Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:55:57.653931Z",
     "start_time": "2021-04-22T17:55:57.608626Z"
    }
   },
   "outputs": [],
   "source": [
    "# something is weird with this column\n",
    "cols = df.columns.tolist()\n",
    "cols.index('saleIs_year_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# something is weird with this column\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('saleIs_year_start')\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Spearman rank-order correlation matrix of the explainatory variables using the `scipy` package. Each coefficient is a measure of the monotonicity of the relationship between two columns : it varies between -1 and +1, with 0 implying no correlation and correlations of -1 or +1 implying an exact monotonic relationship. Positive correlations imply that as x increases, so does y, whereas negative correlations imply that as x increases, y decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:56:18.600071Z",
     "start_time": "2021-04-22T17:56:15.809111Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = np.round(scipy.stats.spearmanr(df[cols]).correlation, 4)\n",
    "corr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T17:56:20.722935Z",
     "start_time": "2021-04-22T17:56:20.680240Z"
    }
   },
   "outputs": [],
   "source": [
    "corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col_small = [\n",
    "    'Transmission',\n",
    "    'Turbocharged',\n",
    "    'Blade_Extension',\n",
    "    'Blade_Width',\n",
    "    'Enclosure_Type',\n",
    "    'Engine_Horsepower',\n",
    "    'Hydraulics',\n",
    "    'Pushblock',\n",
    "]\n",
    "corr_small = np.round(scipy.stats.spearmanr(df[col_small]).correlation, 4)\n",
    "corr_small.size\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr_small)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels([''] + col_small)\n",
    "ax.set_yticklabels([''] + col_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr with size = (65, 65) -> corr_condensed with size (65 * 64)/2\n",
    "corr_condensed = hierarchy.distance.squareform(1 - corr)\n",
    "corr_condensed.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(65 * 64)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_linkage = hierarchy.linkage(corr_condensed, method = 'average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute plot\n",
    "fig = plt.figure(figsize = (16,20))\n",
    "dendrogram = hierarchy.dendrogram(\n",
    "    corr_linkage, \n",
    "    labels = cols, \n",
    "    orientation = 'left', \n",
    "    leaf_font_size = 16,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hierarchical clustering\n",
    "threshold = 0.1\n",
    "cluster_ids = hierarchy.fcluster(corr_linkage, threshold, criterion = 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape clustering as {cluster_index : feature_index}\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_id_to_feature_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only first feature for each cluster\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "# shift feature index after 62\n",
    "selected_features = [(i if i < 62 else i+1) for i in selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retained set of features\n",
    "selected_features = [df.columns[i] for i in selected_features]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform training on ablated set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keep = df[selected_features].copy()\n",
    "\n",
    "n_total = len(df_keep)\n",
    "n_valid = 12000  # same as Kaggle's test set size\n",
    "n_train = n_total - n_valid\n",
    "n_small = 20000\n",
    "\n",
    "X_train_keep, X_valid_keep = split_vals(df_keep, n_train)\n",
    "y_train, y_valid = split_vals(y, n_train)\n",
    "\n",
    "X_small_keep, _ = split_vals(df_keep, n_small)\n",
    "y_small, _ = split_vals(y, n_small)\n",
    "\n",
    "print('Number of small training data points: X = {}, y = {}'.format(X_small.shape, y_small.shape))\n",
    "print('Number of full training data points: X = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
    "print('Number of validation data points: X = {}, y = {}'.format(X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_estimators = 50,\n",
    "    max_depth = 15,\n",
    "    min_samples_split = 10,\n",
    "    max_features = 0.5, # default = 'auto'/None\n",
    "    n_jobs = -1, \n",
    "    random_state = 42,\n",
    "    \n",
    "    bootstrap = True,\n",
    "    oob_score = True, # default = False, \n",
    "    max_samples = 0.75, # default = None,\n",
    ")\n",
    "\n",
    "%time model.fit(X_train_keep, y_train)\n",
    "print_score(model, X_train_keep, y_train, X_valid_keep, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance does not improve. Let us now see if some features shows up as important and wasn't before due to the correlation issues :\n",
    "\n",
    "#### Permutation of feature importance after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model, \n",
    "    X_valid_keep, \n",
    "    y_valid, \n",
    "    n_repeats = 5,\n",
    "    random_state = 42, \n",
    "    n_jobs = -1,\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 50))\n",
    "ax.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert = False, \n",
    "    labels = X_valid.columns[sorted_idx],\n",
    ")\n",
    "ax.set_title(\"Permutation Importances (validation set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : We now see that `Blade_Extension`overtakes `YearMade`as the most discriminant variable. This makes a lot of sense since according to the previous dendrogram this features is extremely correlated with the feature `Engine_Horsepower`, which indeed must heavily impact the sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature_explicability'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Individual feature explicability\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "installs requiered:\n",
    "- `pip install plotnine`\n",
    "- `conda install matplotlib=2.2.3`\n",
    "\n",
    "#### Partial Dependance Plots of explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_sample(df_raw[df_raw.YearMade > 1930], 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(stat_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ggplot(sample, aes('YearMade', 'SalePrice')) + stat_smooth(se = True, method = 'auto')\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Conditional Expectation plots of explanatory variables\n",
    "\n",
    "\n",
    "Individual Conditional Expectation (ICE) plots display one line per instance that shows how the instance's prediction changes when a feature changes. The values for a line (and one instance) can be computed by keeping all other features the same, creating variants of this instance by replacing the feature's value with values from a grid and making predictions with the black box model for these newly created instances. The result is a set of points for an instance with the feature value from the grid and the respective predictions. An ICE plot is more precise than a partial dependence plot as the latter does not focus on specific instances, but on an overall average, which can obscure a heterogeneous relationship created by interactions.\n",
    "\n",
    "See more on ICE plots [here](https://christophm.github.io/interpretable-ml-book/ice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = get_sample(X_train[X_train.YearMade > 1930], 500)\n",
    "x = get_sample(df_keep[df_keep.YearMade > 1930], 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdp(model, x, feat, clusters = None, feat_name = None):\n",
    "    feat_name = feat_name or feat\n",
    "    p = pdp.pdp_isolate(model, x, feat)\n",
    "    return pdp.pdp_plot(p, feat_name, plot_lines=True,\n",
    "                        cluster=clusters is not None,\n",
    "                        n_cluster_centers=clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdp(model, x, 'YearMade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster 500 lines into 5 clusters\n",
    "plot_pdp(model, x, 'YearMade', clusters = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feats = ['saleYear', 'YearMade']\n",
    "p = pdp.pdp_interact(model, x, feats)\n",
    "pdp.pdp_interact_plot(p, feats)\n",
    "# handle error of gdp_plot updates: \n",
    "# https://forums.fast.ai/t/pbpbox-error-on-pdp-interact-plot/28468/2\n",
    "# in C:\\Users\\Jb\\.conda\\envs\\fastai-cpu-v0.7\\Lib\\site-packages\\pdpbox/pdp.py\n",
    "# Line 1147 change \n",
    "# ax.clabel(c2, contour_label_fontsize=9, inline=1) \n",
    "# to ax.clabel(c2, fontsize=9, inline=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF4lXdczybA6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8vGd8_9ybA6"
   },
   "source": [
    "[Back to top](#plan)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "colab": {
   "collapsed_sections": [
    "CfkT54i6ybAV",
    "mOgJutQCybA3"
   ],
   "name": "Machine_Learning_Spotify_2-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
